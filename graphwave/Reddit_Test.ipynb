{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Reddit-Monthly-w/-Graphwave\" data-toc-modified-id=\"Reddit-Monthly-w/-Graphwave-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reddit Monthly w/ Graphwave</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-only-T-1-embedding\" data-toc-modified-id=\"Using-only-T-1-embedding-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Using only T-1 embedding</a></span></li><li><span><a href=\"#Pure-Sum\" data-toc-modified-id=\"Pure-Sum-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pure Sum</a></span></li><li><span><a href=\"#Expotential-Sum\" data-toc-modified-id=\"Expotential-Sum-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Expotential Sum</a></span></li></ul></li><li><span><a href=\"#Reddit-Weekly\" data-toc-modified-id=\"Reddit-Weekly-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reddit Weekly</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-only-T-1-Embedding\" data-toc-modified-id=\"Using-only-T-1-Embedding-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Using only T-1 Embedding</a></span></li><li><span><a href=\"#Pure-Sum\" data-toc-modified-id=\"Pure-Sum-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pure Sum</a></span></li><li><span><a href=\"#Exponential-Sum\" data-toc-modified-id=\"Exponential-Sum-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Exponential Sum</a></span></li></ul></li><li><span><a href=\"#Reddit-Equal\" data-toc-modified-id=\"Reddit-Equal-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Reddit Equal</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-only-T-1-Embedding\" data-toc-modified-id=\"Using-only-T-1-Embedding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Using only T-1 Embedding</a></span></li><li><span><a href=\"#Pure-Sum\" data-toc-modified-id=\"Pure-Sum-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Pure Sum</a></span></li><li><span><a href=\"#Exponential-Sum\" data-toc-modified-id=\"Exponential-Sum-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Exponential Sum</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import graphwave\n",
    "from graphwave.shapes import build_graph\n",
    "from graphwave.graphwave import *\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_features(node_emb_1, node_emb_2, operator):\n",
    "    \n",
    "    # combine two nodes' embeddings with specificed operator\n",
    "    if operator == 'Average':\n",
    "        edge = [((x + y) / 2.0) for x,y in zip(node_emb_1, node_emb_2)]\n",
    "    elif operator == 'Hadamard':\n",
    "        edge = [(x * y) for x,y in zip(node_emb_1, node_emb_2)]\n",
    "    elif operator == 'Weighted-L1':\n",
    "        edge = [abs(x - y) for x,y in zip(node_emb_1, node_emb_2)]\n",
    "    elif operator == 'Weighted-L2':\n",
    "        edge = [abs(x - y)**2 for x,y in zip(node_emb_1, node_emb_2)]\n",
    "    else:\n",
    "        print(\"Generate edge features: Operator not supported\")\n",
    "        print(\"Use default operator: Weighted-L1\")\n",
    "        edge = [abs(x - y) for x,y in zip(node_emb_1, node_emb_2)]\n",
    "        \n",
    "    return edge\n",
    "def generate_edge_features(edge_list, node_embeddings, operator):\n",
    "    edge_features_mtx = []\n",
    "    \n",
    "    # generate features for each edge in the list\n",
    "    for node_index_1, node_index_2 in edge_list:\n",
    "        node_emb_1 = node_embeddings[node_index_1]\n",
    "        node_emb_2 = node_embeddings[node_index_2]\n",
    "        \n",
    "        edge_features_mtx.append(edge_features(node_emb_1, node_emb_2, operator))\n",
    "        \n",
    "    return edge_features_mtx\n",
    "\n",
    "def generate_train_set(graph_train, num_edge_sample, node_embeddings, edge_operator,):\n",
    "    edge_list = list(graph_train.edges)\n",
    "    num_nodes = graph_train.number_of_nodes()\n",
    "    \n",
    "    train_edges = []\n",
    "    train_edges_labels = [1] * num_edge_sample + [0] * num_edge_sample\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # sample edges with label 1 (true edges)\n",
    "    for edge_num in range(num_edge_sample):\n",
    "        rand_index = random.randint(0, len(edge_list) - 1)\n",
    "        \n",
    "        #train_edges.append(tuple(edge_list[rand_index]))\n",
    "        train_edges.append(edge_list[rand_index])\n",
    "    non_edge_num = 0\n",
    "    \n",
    "    # sample edges with label 0 (non-exist edges)\n",
    "    while(non_edge_num < num_edge_sample):\n",
    "        rand_nodes = tuple(np.random.randint(low=0,high=num_nodes, size=2))\n",
    "        \n",
    "        if rand_nodes not in edge_list:\n",
    "            train_edges.append(rand_nodes)\n",
    "            non_edge_num += 1\n",
    "\n",
    "    train_edges_features_mtx = generate_edge_features(train_edges, node_embeddings, edge_operator)\n",
    "            \n",
    "    return train_edges, train_edges_features_mtx, train_edges_labels\n",
    "\n",
    "def generate_test_set(graph_test, node_embeddings, edge_operator):\n",
    "    edge_list = graph_test.edges\n",
    "    nodes_with_edge = set()\n",
    "    \n",
    "    for edge in edge_list:\n",
    "        nodes_with_edge.add(edge[0])\n",
    "        nodes_with_edge.add(edge[1])\n",
    "    \n",
    "    num_nodes = graph_test.number_of_nodes()\n",
    "    \n",
    "    test_edges = []\n",
    "    test_edges_labels = []\n",
    "    \n",
    "    num_edge_sample = len(edge_list)\n",
    "    non_edge_num = 0 \n",
    "    # sample edges with label 0 (non-exist edges)\n",
    "    \n",
    "    while(non_edge_num < num_edge_sample):\n",
    "        rand_nodes = tuple(np.random.randint(low=0,high=num_nodes, size=2))\n",
    "        \n",
    "        if rand_nodes not in edge_list:\n",
    "            test_edges.append(rand_nodes)\n",
    "            test_edges_labels.append(0)\n",
    "            non_edge_num += 1\n",
    "        \n",
    "    for edge in edge_list:\n",
    "        test_edges.append(edge)\n",
    "        test_edges_labels.append(1)\n",
    "    '''\n",
    "    # generate all possible edges for each node with at least one edge (assume undirected edges)\n",
    "    for node_1 in nodes_with_edge:\n",
    "        for node_2 in range(num_nodes):\n",
    "            test_edges.append((node_1, node_2))\n",
    "            \n",
    "            if (node_1, node_2) in edge_list:\n",
    "                test_edges_labels.append(1)\n",
    "            else:\n",
    "                test_edges_labels.append(0)\n",
    "    '''\n",
    "    test_edges_features_mtx = generate_edge_features(test_edges, node_embeddings, edge_operator)\n",
    "    \n",
    "    return test_edges, test_edges_features_mtx, test_edges_labels\n",
    "\n",
    "def build_clf(feature_mtx, response_vec):\n",
    "   \n",
    "    logistic_regression_model = LogisticRegression(random_state = 0,max_iter=5000,solver='liblinear',verbose=1,tol=1e-6)\n",
    "    binary_clf = logistic_regression_model.fit(feature_mtx, response_vec)\n",
    "    \n",
    "    return binary_clf\n",
    "\n",
    "def pred_links(feature_mtx, LR_clf):\n",
    "    predict_edges_labels = LR_clf.predict(feature_mtx)\n",
    "    \n",
    "    return predict_edges_labels\n",
    "\n",
    "def precision_recall(predict_labels, true_labels):\n",
    "    true_positive  = false_positive = 0\n",
    "    true_negative =  false_negative = 0\n",
    "    \n",
    "    for p_label, true_label in zip(predict_labels, true_labels):\n",
    "        \n",
    "        #print(p_label,true_label)\n",
    "        if p_label == true_label and true_label == 1:\n",
    "            true_positive += 1\n",
    "        elif p_label == true_label and true_label == 0:\n",
    "            true_negative += 1\n",
    "        elif p_label != true_label and true_label == 1:\n",
    "            false_negative += 1\n",
    "        elif p_label != true_label and true_label == 0:\n",
    "            false_positive += 1\n",
    "\n",
    "    print(\"TP: \", true_positive)\n",
    "    print(\"TN: \", true_negative)\n",
    "    print(\"FP: \", false_positive)\n",
    "    print(\"FN: \", false_negative)\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        print(\"F1: {}\".format(f1))\n",
    "    except:\n",
    "        print(\"F1: Error\")\n",
    "    \n",
    "    cm = metrics.confusion_matrix(true_labels, predict_labels)\n",
    "    print(cm)\n",
    "    print(metrics.classification_report(true_labels, predict_labels))\n",
    "    map = metrics.average_precision_score(true_labels, predict_labels)\n",
    "    print('Mean Average Precision: {}'.format(map))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_labels, predict_labels)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print('Area Under ROC Curve: {}'.format(roc_auc))\n",
    "    \n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Monthly w/ Graphwave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the graphs \n",
    "with open('/z/pujat/576/data/reddit/reddit_1_month_dir.pkl', 'rb') as file:\n",
    "    graphs = pickle.load(file)\n",
    "graph_train = graphs[-2]\n",
    "graph_test = graphs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_list = []\n",
    "heat_print_list = []\n",
    "taus_list = []\n",
    "for e, g in enumerate(graphs[:-1]): #last embedding used for link prediction\n",
    "    chi, heat_print, taus = graphwave_alg(g, np.linspace(0,128,50), taus='auto', verbose=True)\n",
    "    chi_list.append(chi)\n",
    "    heat_print_list.append(heat_print)\n",
    "    taus_list.append(taus)\n",
    "    print(\"Completed: {}/{}\".format(e,len(graphs[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only T-1 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edge_sample = 400\n",
    "edge_operator = 'Average' #'Average', 'Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, chi_list[-2], edge_operator)\n",
    "test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, chi_list[-1], edge_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "    train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, chi_list[-2], edge_operator)\n",
    "    test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, chi_list[-1], edge_operator)\n",
    "\n",
    "    LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "    print(\"Edge Operator: {}\".format(edge_operator))\n",
    "    predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "    precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_embedding = np.sum(np.asarray(chi_list[0:-1]),axis=0)\n",
    "cur_embedding = np.sum(np.asarray(chi_list),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edge_sample = 400\n",
    "#Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "    train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "    test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "\n",
    "    LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "    print(\"Edge Operator: {}\".format(edge_operator))\n",
    "    predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "    precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expotential Sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = chi_list\n",
    "num_edge_sample = 400\n",
    "#Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "for decay in [1,0.9,0.5,0.3]:\n",
    "    print(\"------------ BEGIN: {} ---------------\".format(decay))\n",
    "    exps = [math.pow(math.e , (-i * decay)) for i in range(1,len(embeddings[:-2]))]\n",
    "    exps.reverse()\n",
    "    temp_embedding = np.zeros((embeddings[0]).shape) \n",
    "    for c,e in zip(embeddings[0:-2],exps):\n",
    "         temp_embedding += e * c \n",
    "    prev_embedding = temp_embedding + embeddings[-2]\n",
    "    \n",
    "    # this is done so the last embedding has weight one. \n",
    "    cur_embedding = temp_embedding + exps[-1] * embeddings[-2] + embeddings[-1]\n",
    "    \n",
    "    for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "        try:\n",
    "            train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "            test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "\n",
    "            LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "            print(\"Edge Operator: {}\".format(edge_operator))\n",
    "            predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "            precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "            print('Precision: ', precision)\n",
    "            print('Recall: ', recall)\n",
    "        except:\n",
    "            print(\"Edge Operator: {} ERROR\".format(edge_operator))\n",
    "    print(\"------------ END: {} ---------------\".format(decay))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the graphs \n",
    "with open('/z/pujat/576/data/reddit/reddit_1_week_undir.pkl', 'rb') as file:\n",
    "    graphs = pickle.load(file)\n",
    "graph_train = graphs[-2]\n",
    "graph_test = graphs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_list = []\n",
    "heat_print_list = []\n",
    "taus_list = []\n",
    "for e, g in enumerate(graphs[:-1]): #last embedding used for link prediction\n",
    "    chi, heat_print, taus = graphwave_alg(g, np.linspace(0,200,50), taus='auto', verbose=True)\n",
    "    chi_list.append(chi)\n",
    "    heat_print_list.append(heat_print)\n",
    "    taus_list.append(taus)\n",
    "    print(\"Completed: {}/{}\".format(e,len(graphs[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only T-1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "    try:\n",
    "        train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, chi_list[-2], edge_operator)\n",
    "        test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, chi_list[-1], edge_operator)\n",
    "\n",
    "        LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "        print(\"Edge Operator: {}\".format(edge_operator))\n",
    "        predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "        precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "        print('Precision: ', precision)\n",
    "        print('Recall: ', recall)\n",
    "    except:\n",
    "        print(\"Edge Operator: {} ERROR\".format(edge_operator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_embedding = np.sum(np.asarray(chi_list[0:-1]),axis=0)\n",
    "cur_embedding = np.sum(np.asarray(chi_list),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edge_sample = 400\n",
    "#Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "    train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "    test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "\n",
    "    LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "    print(\"Edge Operator: {}\".format(edge_operator))\n",
    "    predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "    precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = chi_list\n",
    "num_edge_sample = 400\n",
    "#Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "for decay in [1,0.9,0.5,0.3]:\n",
    "    print(\"------------ BEGIN: {} ---------------\".format(decay))\n",
    "    exps = [math.pow(math.e , (-i * decay)) for i in range(1,len(embeddings[:-2]))]\n",
    "    exps.reverse()\n",
    "    temp_embedding = np.zeros((embeddings[0]).shape) \n",
    "    for c,e in zip(embeddings[0:-2],exps):\n",
    "         temp_embedding += e * c \n",
    "    prev_embedding = temp_embedding + embeddings[-2]\n",
    "    \n",
    "    # this is done so the last embedding has weight one. \n",
    "    cur_embedding = temp_embedding + exps[-1] * embeddings[-2] + embeddings[-1]\n",
    "    \n",
    "    for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "        try:\n",
    "            train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "            test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "\n",
    "            LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "            print(\"Edge Operator: {}\".format(edge_operator))\n",
    "            predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "            precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "            print('Precision: ', precision)\n",
    "            print('Recall: ', recall)\n",
    "        except:\n",
    "            print(\"Edge Operator: {} ERROR\".format(edge_operator))\n",
    "    print(\"------------ END: {} ---------------\".format(decay))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the graphs \n",
    "with open('/z/pujat/576/data/reddit/reddit_equal_monthly_dir.pkl', 'rb') as file:\n",
    "    graphs = pickle.load(file)\n",
    "graph_train = graphs[-2]\n",
    "graph_test = graphs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_list = []\n",
    "heat_print_list = []\n",
    "taus_list = []\n",
    "for e, g in enumerate(graphs[:-1]): #last embedding used for link prediction\n",
    "    chi, heat_print, taus = graphwave_alg(g, np.linspace(0,200,50), taus='auto', verbose=True)\n",
    "    chi_list.append(chi)\n",
    "    heat_print_list.append(heat_print)\n",
    "    taus_list.append(taus)\n",
    "    print(\"Completed: {}/{}\".format(e,len(graphs[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using only T-1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edge_sample = 400\n",
    "edge_operator = 'Average' #'Average', 'Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, chi_list[-2], edge_operator)\n",
    "test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, chi_list[-1], edge_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "    train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, chi_list[-2], edge_operator)\n",
    "    test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, chi_list[-1], edge_operator)\n",
    "\n",
    "    LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "    print(\"Edge Operator: {}\".format(edge_operator))\n",
    "    predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "    precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_embedding = np.sum(np.asarray(chi_list[0:-1]),axis=0)\n",
    "cur_embedding = np.sum(np.asarray(chi_list),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edge_sample = 400\n",
    "#Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "    train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "    test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "\n",
    "    LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "    print(\"Edge Operator: {}\".format(edge_operator))\n",
    "    predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "    precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = chi_list\n",
    "num_edge_sample = 400\n",
    "#Average' or 'Hadamard' or 'Weighted-L1' or 'Weighted-L2'\n",
    "for decay in [1,0.9,0.5,0.3]:\n",
    "    print(\"------------ BEGIN: {} ---------------\".format(decay))\n",
    "    exps = [math.pow(math.e , (-i * decay)) for i in range(1,len(embeddings[:-2]))]\n",
    "    exps.reverse()\n",
    "    temp_embedding = np.zeros((embeddings[0]).shape) \n",
    "    for c,e in zip(embeddings[0:-2],exps):\n",
    "         temp_embedding += e * c \n",
    "    prev_embedding = temp_embedding + embeddings[-2]\n",
    "    \n",
    "    # this is done so the last embedding has weight one. \n",
    "    cur_embedding = temp_embedding + exps[-1] * embeddings[-2] + embeddings[-1]\n",
    "    \n",
    "    for edge_operator in ['Average','Hadamard','Weighted-L1','Weighted-L2']:\n",
    "        try:\n",
    "            train_edges, train_edges_features_mtx, train_edges_labels = generate_train_set(graph_train, num_edge_sample, prev_embedding, edge_operator)\n",
    "            test_edges, test_edges_features_mtx, test_edges_labels = generate_test_set(graph_test, cur_embedding, edge_operator)\n",
    "\n",
    "            LR_clf = build_clf(train_edges_features_mtx, train_edges_labels)\n",
    "\n",
    "            print(\"Edge Operator: {}\".format(edge_operator))\n",
    "            predict_edges_labels = pred_links(test_edges_features_mtx, LR_clf)\n",
    "            precision, recall = precision_recall(list(predict_edges_labels), list(test_edges_labels))\n",
    "            print('Precision: ', precision)\n",
    "            print('Recall: ', recall)\n",
    "        except:\n",
    "            print(\"Edge Operator: {} ERROR\".format(edge_operator))\n",
    "    print(\"------------ END: {} ---------------\".format(decay))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
